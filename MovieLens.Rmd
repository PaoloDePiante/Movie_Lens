---
title: "MovieLens Data Analysis"
author: "Paolo De Piante"
date: "`r format(Sys.Date())`"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center', out.width = '90%') 
```


# 1. Introduction

This is a report on MovieLens 10M dataset generated by the GroupLens research lab. 
The dataset can be downloaded from [this weblink](http://files.grouplens.org/datasets/movielens/ml-10m.zip).
The sdudy has been inspired by HarvardX & edX "Data Science: Capstone" MOOC. The gaol is to test some Machine Learing algorithms and approches referred to the domain named "Recommandation Systems". RMSE will be used to evaluate how the model trained is far from predictions. The main challenge to address is to find the lowest RMSE to compare and to identify  the better method to fill in the sparse matrix with the movie ratings closer to each single user profile. Just to know, the highest predicted movie ratings are used to suggest some movies not voted or never seen by a specific user yet. However, this last goal is not part of this study. 
Sereral methods are available but some of them are not feasible considering the hardware used for this study or the time required to sdudy all of them is very time consuming. So some methods have been picked up hoping to find a good one betwen them.

_Note: some code lines comes from lessons provided by Professor Rafael Irizarry in his Data Science Certification Program provided by edX._


# 2. Analysis

## 2.1 Data wrangling 

We are going to use the following library:

```{r recall libraies, message=FALSE}
library(tidyverse)
library(lubridate)
library(Matrix)
library(recommenderlab)
library(Matrix.utils)
library(irlba)
library(recosystem)
```


Basically two main data partitions will be used: "edx" to train and "validation" to validate the model. The data partition has been provided by edX "Data Science: Capstone" course mentioned in the introduction section. As follows I report the code provided:

```{r wrangling, message=FALSE, warning=FALSE, results='hide'}
if(!require(tidyverse)) install.packages("tidyverse", 
                                         repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", 
                                     repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", 
                                          repos = "http://cran.us.r-project.org")



# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip


dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)


ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))


movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")


# Validation set will be 10% of MovieLens data

set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]


# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")


# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
```

## 2.2 Data exploration

Exploring the edx dataset properties I want to highlight: the dimension of the data matrix (users, movies) = [69878,  10677]; the data frame has 9000055 observations (number of ratings); the sparse matrix has many empty cells (movies not voted); ratings range goes from 0.5 to 5.

```{r basic exploration, message=FALSE, warning=FALSE}
# Exploring the data set edx 
class(edx)
head(edx)
# str(edx) # not reported
dim(edx)
length(unique(edx$movieId))  # number of different movies
length(unique(edx$userId))   # number of differen users
summary(edx)
```

Continuing to explore edx dataset, I used descriptive statistics to analyze some variables as follows:

* **Genres**: for each genre, has been counted the number of ratings. It seems the Genres has effects on rating distribution. Also condidering classes of genres by the average ratings, the effect is confirmed;
* **Time**: there is no marked evidence of a time effect on average rating. It is mostly stable also considering different time frame (weeks, months, etc.). I decide to exclude "Timestamp" in my analysis;
* **Ratings**: it seems that both, movies and users, have effect on rating distribution. There are movies with many votes and movies with a low rate of votes. There are users that usually provide a vote to the movies they see and on the other hand there are users that provide very few votes; 
* **Movies mostly rated**: accordingly to previous point I just obtained the fourty movies most voted. 


```{r exploration, message=FALSE, warning=FALSE, results='hide'}
# Explore Genres

Genres <- edx %>% separate_rows(genres, sep = "\\|") %>% group_by(genres) %>% 
  summarize(count = n()) %>% arrange(desc(count))
Genres %>% ggplot(aes(x=reorder(genres, count), y=count)) + 
  geom_bar(stat="identity", fill="blue") +
  coord_flip(y=c(0, 4500000)) +
  labs(x="Genres", y="Number of Movies per Genres") +
  scale_y_continuous(breaks = c(0,400000,1000000,2000000,4500000)) +
  geom_text(aes(label=count), hjust=-0.2, size=3)

# Explore the Genres by the average ratings. 

edx %>% group_by(genres) %>%
  summarize(n = n(), avg = mean(rating), se = sd(rating)/sqrt(n())) %>%
  filter(n >= 50000) %>% 
  mutate(genres = reorder(genres, avg)) %>%
  ggplot(aes(x = genres, y = avg, ymin = avg - 2*se, ymax = avg + 2*se)) + 
  geom_point() +
  geom_errorbar() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Explore Time 

Times <- mutate(edx, date = as_datetime(timestamp))
Times %>% mutate(date = round_date(date, unit = "week")) %>%
  group_by(date) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(date, rating)) +
  geom_point() +
  geom_smooth()

# Explore number of ratings by movieId and UserId. 

edx %>%  count(movieId) %>% ggplot(aes(n)) +
  geom_histogram(bins=30, color="white") +
  scale_x_log10() +
  ggtitle("Movies") +
  labs(x="n in 30 bins", y="Count in bins") 
  
  edx %>% count(userId) %>% ggplot(aes(n)) +
  geom_histogram(bins=30, color="white") +
  scale_x_log10() +
  ggtitle("Users") +
  labs(x="n in 30 bins", y="Count in bins") 
  
  # Explore the Movies with highest number of ratings. 

Movie_rating_count <- edx %>% group_by(title) %>% summarize(count=n()) %>% 
  top_n(40, count) %>% arrange(desc(count))
Movie_rating_count %>% ggplot(aes(x=reorder(title, count), y=count)) + 
  geom_bar(stat="identity", fill="blue") +
  coord_flip(y=c(0, 40000)) +
  labs(x="Top Movies", y="Number of Ratings") +
  scale_y_continuous(breaks = c(0,10000,20000,30000,40000)) +
  geom_text(aes(label=count), hjust=-0.2, size=3) 
  
```

## 2.3 Pre-processing: defining RMSE function

Before to go through the analysis, I define the "Root Mean Square Error" function that will be used to evaluate how good the model fits with the validation data.

```{r RMSE, message=FALSE, warning=FALSE, results='hide'}
# Function Definition for RMSE calculation

RMSE <- function(true_rating, predicted_rating){
  sqrt(mean((true_rating - predicted_rating)^2))}
```

## 2.4 Pre-processing: Building a sparse matrix for data analysis

I am going to implement the sparseMatrix function from Matrix library to build the matrix. Before to do that I:

* copied the original edX data frame to leave it available for further uses; 
* transformed userId and movieId as factor data type. I do that because factors are used to group the units under observation (in this case users and movies) and this is required by further analysis.

After that, starting from the sparse matrix, I created a "realRatingMatrix" object from recommanderlab package.
Just to see how does it appear the sparse matrix I generate an image of it for only 100 x 100 cells.
 

```{r sparse matrix, message=FALSE, warning=FALSE, results='hide'}
# Obtain a sparse Matrix in class realRatingMatrix 

edx2 <- edx
edx2$userId <- as.factor(edx2$userId)
edx2$movieId <- as.factor(edx2$movieId)

edx2$userId <- as.numeric(edx2$userId)
edx2$movieId <- as.numeric(edx2$movieId)

sparse_ratings <- sparseMatrix(i = edx2$userId, j = edx2$movieId, x = edx2$rating,
                               dims = c(length(unique(edx2$userId)), 
                                        length(unique(edx2$movieId))))

ratingMat <- new("realRatingMatrix", data = sparse_ratings)
image(ratingMat[1:100, 1:100])

```

## 2.4 Facing with dimensionality issues       

Sparsity of ratings and the high number of 'NULL' values highlights a very challenging problem in terms of resources required to calculate a single algorithms or a bunch of them as an ensable. There are two main technics to try to reduce the dimension of the sparse matrix: “PCA” and “SVD”. Looking at svd method I meet the Irlba package that implement a memory-efficient way to compute a partial SVD. According to what was suggested by Rafael Irizarry into the Matrix factorization lesson, the SVD can help us to decompose a Matrix M[R,C] with R<C in as follows:

                                                    M = U x D x V'
where
     * U  : orthogonal matrix of dimensions R x m 
     * D  : diagonal matrix containing the singular values of the original matrix, m x m 
     * V' : orthogonal matrix transposed of dimensions m x C

Irlba algorithm shows the k parameter equal to the number of Singular vectors with 90% of variability explained. It is also shown into the plot. In this way we could say that our original matrix made of 9000055 ratings could become of (69878∗55)+(55∗55)+(55∗10677)=4,433,550 cells. So more or less the 50% less than the original one. I tried to use this matrix ma is still quite big for my hardware. So this is a key point to address. How to reduce the sparse matrix without loose important data to train the model? I also tried PCA method but I will tell about it further in this study.   

```{r SVD Matrix reduction, message=FALSE, warning=FALSE}
set.seed(1)
Red_ratings <- irlba(sparse_ratings, tol=1e-4, verbose=TRUE, nv= 100, maxit=1000)
plot(cumsum(Red_ratings$d^2/sum(Red_ratings$d^2)), type="l", xlab="SIngular Vector",
     ylab="Variability Explained Cumulated")
lines(x=c(0,100), y= c(.90, .90))
k = max(which(cumsum(Red_ratings$d^2/sum(Red_ratings$d^2)) <= .90))
k # Number of Singular vectors with 90% of variability explained
k = 100
U <- Red_ratings$u[, 1:k]
D <- Diagonal(x = Red_ratings$d[1:k])
V <- t(Red_ratings$v)[1:k,]

Red_ratings_svd <- U%*%D%*%V # This provide us the original matrix at 90%

p  <- U%*%V # this provide the predicted ratings
```

