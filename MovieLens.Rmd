---
title: "MovieLens Data Analysis"
author: "Paolo De Piante"
date: "`r format(Sys.Date())`"
output:
  pdf_document: default
  latex_engine: lualatex
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center', out.width = '90%') 

```


# 1. Introduction

This is a report on MovieLens 10M dataset generated by the GroupLens research lab. 
The dataset can be downloaded from [this weblink](http://files.grouplens.org/datasets/movielens/ml-10m.zip).
The sdudy has been inspired by HarvardX & edX "Data Science: Capstone" MOOC. The gaol is to test some Machine Learing algorithms and approches referred to the domain named "Recommandation Systems". RMSE will be used to evaluate how the model trained is far from predictions. The main challenge to address is to find the lowest RMSE to compare and to identify  the better method to fill in the sparse matrix with the movie ratings closer to each single user profile. Just to know, the highest predicted movie ratings are used to suggest some movies not voted or never seen by a specific user yet. However, this last goal is not part of this study. 
Sereral methods are available but some of them are not feasible considering the hardware used for this study or the time required to sdudy all of them is very time consuming. So some methods have been picked up hoping to find a good one betwen them.

_Note: some code lines comes from lessons provided by Professor Rafael Irizarry in his Data Science Certification Program provided by edX._


# 2. Analysis

## 2.1 Data wrangling 

We are going to use the following library:

```{r recall libraies, message=FALSE}
library(tidyverse)
library(lubridate)
library(Matrix)
library(recommenderlab)
library(Matrix.utils)
library(irlba)
library(recosystem)
```


Basically two main data partitions will be used: "edx" to train and "validation" to validate the model. The data partition has been provided by edX "Data Science: Capstone" course mentioned in the introduction section. As follows I report the code provided:

```{r wrangling, message=FALSE, warning=FALSE, results='hide'}
if(!require(tidyverse)) install.packages("tidyverse", 
                                         repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", 
                                     repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", 
                                          repos = "http://cran.us.r-project.org")



# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip


dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)


ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))


movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")


# Validation set will be 10% of MovieLens data

set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]


# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")


# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
```

## 2.2 Data exploration

Exploring the edx dataset properties I want to highlight: the dimension of the data matrix (users, movies) = [69878,  10677]; the data frame has 9000055 observations (number of ratings); the sparse matrix has many empty cells (movies not voted); ratings range goes from 0.5 to 5.

```{r basic exploration, message=FALSE, warning=FALSE}
# Exploring the data set edx 
class(edx)
head(edx)
# str(edx) # not reported
dim(edx)
length(unique(edx$movieId))  # number of different movies
length(unique(edx$userId))   # number of differen users
summary(edx)
```

Continuing to explore edx dataset, I used descriptive statistics to analyze some variables as follows:

* **Genres**: for each genre, has been counted the number of ratings. It seems the Genres has effects on rating distribution. Also condidering classes of genres by the average ratings, the effect is confirmed;
* **Time**: there is no marked evidence of a time effect on average rating. It is mostly stable also considering different time frame (weeks, months, etc.). I decide to exclude "Timestamp" in my analysis;
* **Ratings**: it seems that both, movies and users, have effect on rating distribution. There are movies with many votes and movies with a low rate of votes. There are users that usually provide a vote to the movies they see and on the other hand there are users that provide very few votes; 
* **Movies mostly rated**: accordingly to previous point I just obtained the fourty movies most voted. 


```{r exploration, message=FALSE, warning=FALSE, results='hide'}
# Explore Genres

Genres <- edx %>% separate_rows(genres, sep = "\\|") %>% group_by(genres) %>% 
  summarize(count = n()) %>% arrange(desc(count))
Genres %>% ggplot(aes(x=reorder(genres, count), y=count)) + 
  geom_bar(stat="identity", fill="blue") +
  coord_flip(y=c(0, 4500000)) +
  labs(x="Genres", y="Number of Movies per Genres") +
  scale_y_continuous(breaks = c(0,400000,1000000,2000000,4500000)) +
  geom_text(aes(label=count), hjust=-0.2, size=3)

# Explore the Genres by the average ratings. 

edx %>% group_by(genres) %>%
  summarize(n = n(), avg = mean(rating), se = sd(rating)/sqrt(n())) %>%
  filter(n >= 50000) %>% 
  mutate(genres = reorder(genres, avg)) %>%
  ggplot(aes(x = genres, y = avg, ymin = avg - 2*se, ymax = avg + 2*se)) + 
  geom_point() +
  geom_errorbar() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Explore Time 

Times <- mutate(edx, date = as_datetime(timestamp))
Times %>% mutate(date = round_date(date, unit = "week")) %>%
  group_by(date) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(date, rating)) +
  geom_point() +
  geom_smooth()

# Explore number of ratings by movieId and UserId. 

edx %>%  count(movieId) %>% ggplot(aes(n)) +
  geom_histogram(bins=30, color="white") +
  scale_x_log10() +
  ggtitle("Movies") +
  labs(x="n in 30 bins", y="Count in bins") 
  
  edx %>% count(userId) %>% ggplot(aes(n)) +
  geom_histogram(bins=30, color="white") +
  scale_x_log10() +
  ggtitle("Users") +
  labs(x="n in 30 bins", y="Count in bins") 
  
  # Explore the Movies with highest number of ratings. 

Movie_rating_count <- edx %>% group_by(title) %>% summarize(count=n()) %>% 
  top_n(40, count) %>% arrange(desc(count))
Movie_rating_count %>% ggplot(aes(x=reorder(title, count), y=count)) + 
  geom_bar(stat="identity", fill="blue") +
  coord_flip(y=c(0, 40000)) +
  labs(x="Top Movies", y="Number of Ratings") +
  scale_y_continuous(breaks = c(0,10000,20000,30000,40000)) +
  geom_text(aes(label=count), hjust=-0.2, size=3) 
  
```

## 2.3 Pre-processing: defining RMSE function

Before to go through the analysis, I define the "Root Mean Square Error" function that will be used to evaluate how good the model fits with the validation data. The formula is:

$$ RMSE = \sqrt{\frac{1}{N}\sum(\hat{y}_{u,i}-y_{u,i})^2}$$
where 

* $\hat{y}_{u,i}$ is the predicted values 
* $y_{u,i}$ is the observed value


```{r RMSE, message=FALSE, warning=FALSE, results='hide'}
# Function Definition for RMSE calculation

RMSE <- function(true_rating, predicted_rating){
  sqrt(mean((true_rating - predicted_rating)^2))}
```

## 2.4 Pre-processing: Building a sparse matrix for data analysis

I am going to implement the sparseMatrix function from Matrix library to build the matrix. Before to do that I:

* copied the original edX data frame to leave it available for further uses; 
* transformed userId and movieId as factor data type. I do that because factors are used to group the units under observation (in this case users and movies) and this is required by further analysis.

After that, starting from the sparse matrix, I created a "realRatingMatrix" object from recommanderlab package.
Just to see how does it appear the sparse matrix I generate an image of it for only 100 x 100 cells.
 

```{r sparse matrix, message=FALSE, warning=FALSE, results='hide'}
# Obtain a sparse Matrix in class realRatingMatrix 

edx2 <- edx
edx2$userId <- as.factor(edx2$userId)
edx2$movieId <- as.factor(edx2$movieId)

edx2$userId <- as.numeric(edx2$userId)
edx2$movieId <- as.numeric(edx2$movieId)

sparse_ratings <- sparseMatrix(i = edx2$userId, j = edx2$movieId, x = edx2$rating,
                               dims = c(length(unique(edx2$userId)), 
                                        length(unique(edx2$movieId))))

ratingMat <- new("realRatingMatrix", data = sparse_ratings)
image(ratingMat[1:100, 1:100])

```

## 2.4 Facing with dimensionality issues       

Sparsity of ratings and the high number of 'NULL' values highlights a very challenging problem in terms of resources required to calculate a single algorithms or a bunch of them as an ensable. There are two main technics to try to reduce the dimension of the sparse matrix: “PCA” and “SVD”. Looking at svd method I meet the Irlba package that implement a memory-efficient way to compute a partial SVD. According to what was suggested by Rafael Irizarry into the Matrix factorization lesson, the SVD can help us to decompose a Matrix M[R,C] with R<C in as follows:

$$M = U \times D \times V'$$

where

* U  : orthogonal matrix of dimensions $R \times m$ 
* D  : diagonal matrix containing the singular values of the original matrix, $m \times m$ 
* V' : orthogonal matrix transposed of dimensions $m \times C$

Irlba algorithm shows the k parameter equal to the number of Singular vectors with 90% of variability explained. It is also shown into the plot. In this way we could say that our original matrix made of 9000055 ratings becomes of
$(69878\times55)+(55\times55)+(55\times10677)=4,433,550$ cells. So more or less the 50% less than the original one.Now the question was: is it possible to do better? So this is a key point to address. How to reduce the sparse matrix without loose important data to train the model? I also tried PCA method but I will tell about it further in this study.   

```{r SVD Matrix reduction, message=FALSE, warning=FALSE}
set.seed(1)
Red_ratings <- irlba(sparse_ratings, tol=1e-4, verbose=TRUE, nv= 100, maxit=1000)
plot(cumsum(Red_ratings$d^2/sum(Red_ratings$d^2)), type="l", xlab="Singular Vector",
     ylab="Variability Explained Cumulated")
lines(x=c(0,100), y= c(.90, .90))
k = max(which(cumsum(Red_ratings$d^2/sum(Red_ratings$d^2)) <= .90))
k # Number of Singular vectors with 90% of variability explained
U <- Red_ratings$u[, 1:k]
D <- Diagonal(x = Red_ratings$d[1:k])
V <- t(Red_ratings$v)[1:k,]

# U%*%D%*%V this provide us the original matrix at 90%
# U%*%V this provide the predicted ratings
```

The idea is to reduce the matrix knowing that many users gives few ratings and many movies doesn't have many ratings too. So we could cut the movies and the rows using this criteria. We calculate quantiles and in particular way the 90th percentile that leaves on its left 90% of the elements of the distribution. we do that for both, movies and users. After that, we take in consideration just the row counts (number of movies voted for each user) and column counts (number of users that provided a vote for each movie) of the matrix that are greater than two quantiles.

```{r Matrix reduction based on quantiles, message=FALSE, warning=FALSE}
# Different approach to reduce the original matrix (realRatingMatrix)

min_movies <- quantile(rowCounts(ratingMat),0.9)
min_movies
min_users <- quantile(colCounts(ratingMat), 0.9)
min_users
ratings_movies <- ratingMat[rowCounts(ratingMat) > min_movies, 
                            colCounts(ratingMat) > min_users]
ratings_movies

```

So the new matrix (named ratings_movies) dimension is made of 2,313,148 ratings on 7,452,504 (elements or cells).

# 2.5. Machine Learning implementation

Based on pre-processing data and information we try some Machine Learning methods. For each one the model will be validated using RMSE function.

## 2.5.1 Linear regression models

The first family models are related to the linaer regression ones using movies, users and genres effects. This approach comes from lessons provided by Irizarry, R. in Recommandation Systems topic. We follow an incremental approach starting from a simple linear regression model including the same rating (the general average "mu") for all users and movies plus an error. So the true rating value for the 'u' user and the 'i' movie is:

$$Y_{u,i} = \mu + \epsilon_{u,i}$$
As follows, the code shows how we added (one by one) to this "naive" starting model the following terms:

* b_i is the avegrage ranking form movies i
* b_u is equal to mean(rating - mu - b_i) the user effect
* b_g is equal to mean(rating - mu - b_i - b_u) the genre effect

to enrich the model. For each one, step by step, the rmse has been calculated. The time has not been included because as we saw in exploratory phase it doesn't have a marked effect on ratings.


```{r movie user and genres effect, message=FALSE, warning=FALSE}
# Linear regression models using just the mean, movies, users and genres effect

mu <- mean(edx$rating)
naive_rmse <- RMSE(validation$rating, mu)
naive_rmse

movie_avg <- edx %>% group_by(movieId) %>% summarize(b_i = mean(rating - mu))
predicted_rating <- mu + validation %>% left_join(movie_avg, by = "movieId") %>% .$b_i
movie_rmse <- RMSE(validation$rating, predicted_rating)
movie_rmse

user_avg <- edx %>% left_join(movie_avg, by = "movieId") %>% group_by(userId) %>% 
  summarize(b_u = mean(rating - mu - b_i)) 
predicted_rating <- validation %>% left_join(movie_avg, by = "movieId") %>% 
  left_join(user_avg, by = "userId") %>% 
  mutate(pred = mu + b_i + b_u) %>% .$pred
movie_user_rmse <- RMSE(validation$rating, predicted_rating)
movie_user_rmse

genres_avg <- edx %>% left_join(movie_avg, by = "movieId") %>% 
  left_join(user_avg, by = "userId") %>%  group_by(genres) %>% 
  summarize(b_g = mean(rating - mu - b_i - b_u))
predicted_rating <- validation %>% left_join(movie_avg, by = "movieId") %>% 
  left_join(user_avg, by = "userId") %>% left_join(genres_avg, by="genres") %>%
  mutate(pred = mu + b_i + b_u + b_g) %>% .$pred
movie_genres_rmse <- RMSE(validation$rating, predicted_rating)
movie_genres_rmse

```
### 2.5.1.1 Regularization

The question is: 
